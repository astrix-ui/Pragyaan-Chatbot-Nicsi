# ğŸ§  Company Knowledge Chatbot

A full-stack AI-powered chatbot that answers user queries based solely on uploaded PDFs and semantically scraped company website data. Built using FastAPI, LangChain, Ollama (LLaMA 3), and React.

---

## âš™ï¸ Tech Stack

- **FastAPI** (Python backend)
- **LangChain** + **Ollama (LLaMA 3)** for LLM interaction and embeddings
- **ChromaDB** as the vector store
- **React** frontend (in `/chatbot-app`)
- **PyMuPDF (fitz)** for PDF extraction
- **BeautifulSoup + Selenium** for semantic web scraping

---

## ğŸ“ Project Structure

```
chatbot/
â”œâ”€â”€ api.py                   # FastAPI backend API
â”œâ”€â”€ categorize_to_vector.py # Converts scraped/categorized data into vector embeddings
â”œâ”€â”€ semantic_scraper.py     # Deep crawler + categorizer using LLaMA3
â”œâ”€â”€ company_data.json       # Stores structured content (scraped + PDFs)
â”œâ”€â”€ user_queries.json       # Stores history of user queries
â”œâ”€â”€ chroma_store/           # Folder containing Chroma vector DB
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ chatbot-app/            # React frontend app
â””â”€â”€ README.md               # This file
```

---

## ğŸš€ Getting Started

### ğŸ”§ 1. Backend Setup

```bash
# Create and activate virtual environment
python -m venv venv
venv\Scripts\activate       # On Windows
# or
source venv/bin/activate    # On macOS/Linux

# Install Python dependencies
pip install -r requirements.txt
```

### ğŸ¦™ 2. Run Ollama with LLaMA 3

Make sure [Ollama](https://ollama.com/download) is installed and running:

```bash
ollama run llama3
```

### ğŸ§  3. Start the FastAPI Server

```bash
python api.py
```

This will load `company_data.json` into Chroma vector store and expose the backend API on `http://localhost:8000`.

---

### ğŸŒ 4. Frontend (React)

```bash
cd chatbot-app
npm install
npm start
```

Visit [http://localhost:3000](http://localhost:3000) to start chatting with your data.

---

## ğŸ“„ Uploading PDFs

You can upload PDF documents via the frontend UI. They will be parsed and appended to `company_data.json` and embedded into Chroma vector DB automatically.

---

## ğŸŒ Scraping a Website

To scrape and categorize website content:

```bash
python semantic_scraper.py
```

Then embed that data:

```bash
python categorize_to_vector.py
```

---

## ğŸ” Resetting Vector DB

To clear Chroma vector store and re-embed:

```bash
# Windows
rmdir /s /q chroma_store

# macOS/Linux
rm -rf chroma_store
```

Then rerun `api.py`.

---

## ğŸ“Œ Maintained By

**Ayush Sharma**
